{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6359a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gnssl\\anaconda3\\envs\\myvenv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3700cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415e75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/dataset/cleaned_improved_cicids2017.csv\")\n",
    "#df = pd.read_csv(\"/Users/anchanghun/Downloads/CIC-Dataset/cleaned_improved_cicids2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b5cf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>Total Length of Fwd Packet</th>\n",
       "      <th>Total Length of Bwd Packet</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>ICMP Code</th>\n",
       "      <th>ICMP Type</th>\n",
       "      <th>Total TCP Flow Time</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>119719148</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22509459</td>\n",
       "      <td>17</td>\n",
       "      <td>12685486.0</td>\n",
       "      <td>5.296658e+06</td>\n",
       "      <td>20694308</td>\n",
       "      <td>6499982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>65511209</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1506210</td>\n",
       "      <td>1506210</td>\n",
       "      <td>64004884.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>64004884</td>\n",
       "      <td>64004884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>113976922</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>20447</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>37</td>\n",
       "      <td>76.580524</td>\n",
       "      <td>44.140625</td>\n",
       "      <td>...</td>\n",
       "      <td>10983883</td>\n",
       "      <td>14</td>\n",
       "      <td>25498178.0</td>\n",
       "      <td>1.883305e+07</td>\n",
       "      <td>48523116</td>\n",
       "      <td>5463561</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>67037196</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11034681</td>\n",
       "      <td>11034681</td>\n",
       "      <td>55956316.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>55956316</td>\n",
       "      <td>55956316</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>68045057</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>384</td>\n",
       "      <td>384</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11043596</td>\n",
       "      <td>11043596</td>\n",
       "      <td>56943904.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>56943904</td>\n",
       "      <td>56943904</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protocol  Flow Duration  Total Fwd Packet  Total Bwd packets  \\\n",
       "0         0      119719148               231                  0   \n",
       "1        17       65511209                 6                  6   \n",
       "2        17      113976922               267                  0   \n",
       "3        17       67037196                 8                  8   \n",
       "4        17       68045057                 8                  8   \n",
       "\n",
       "   Total Length of Fwd Packet  Total Length of Bwd Packet  \\\n",
       "0                           0                           0   \n",
       "1                         288                         288   \n",
       "2                       20447                           0   \n",
       "3                         384                         384   \n",
       "4                         384                         384   \n",
       "\n",
       "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
       "0                      0                      0                0.000000   \n",
       "1                     48                     48               48.000000   \n",
       "2                    153                     37               76.580524   \n",
       "3                     48                     48               48.000000   \n",
       "4                     48                     48               48.000000   \n",
       "\n",
       "   Fwd Packet Length Std  ...  Active Max  Active Min   Idle Mean  \\\n",
       "0               0.000000  ...    22509459          17  12685486.0   \n",
       "1               0.000000  ...     1506210     1506210  64004884.0   \n",
       "2              44.140625  ...    10983883          14  25498178.0   \n",
       "3               0.000000  ...    11034681    11034681  55956316.0   \n",
       "4               0.000000  ...    11043596    11043596  56943904.0   \n",
       "\n",
       "       Idle Std  Idle Max  Idle Min  ICMP Code  ICMP Type  \\\n",
       "0  5.296658e+06  20694308   6499982          0          0   \n",
       "1  0.000000e+00  64004884  64004884          0          0   \n",
       "2  1.883305e+07  48523116   5463561          0          0   \n",
       "3  0.000000e+00  55956316  55956316          0          0   \n",
       "4  0.000000e+00  56943904  56943904          0          0   \n",
       "\n",
       "   Total TCP Flow Time   Label  \n",
       "0                    0  BENIGN  \n",
       "1                    0  BENIGN  \n",
       "2                    0  BENIGN  \n",
       "3                    0  BENIGN  \n",
       "4                    0  BENIGN  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6288929f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN                        1594540\n",
       "Portscan                       159066\n",
       "DoS Hulk                       158468\n",
       "DDoS                            95144\n",
       "Infiltration - Portscan         71767\n",
       "DoS GoldenEye                    7567\n",
       "FTP-Patator                      3972\n",
       "DoS Slowloris                    3859\n",
       "SSH-Patator                      2961\n",
       "DoS Slowhttptest                 1740\n",
       "Botnet                            736\n",
       "Web Attack - Brute Force           73\n",
       "Infiltration                       36\n",
       "Web Attack - XSS                   18\n",
       "Web Attack - SQL Injection         13\n",
       "Heartbleed                         11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({'BENIGN': 1594540, 'Portscan': 159066, 'DoS Hulk': 158468, 'DDoS': 95144, 'DoS GoldenEye': 7567, 'FTP-Patator': 3972, 'DoS Slowloris': 3859, 'SSH-Patator': 2961, 'DoS Slowhttptest': 1740})\n",
      "SMOTE Sampling Strategy: {'FTP-Patator': 1195905, 'SSH-Patator': 1195905, 'DoS Slowloris': 1195905, 'DoS Slowhttptest': 1195905, 'DoS Hulk': 1195905, 'DoS GoldenEye': 1195905, 'Portscan': 1195905, 'DDoS': 1195905}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ✅ 사용할 공격 유형 리스트 (BENIGN 포함)\n",
    "selected_classes = [\n",
    "    \"BENIGN\",\n",
    "    \"FTP-Patator\",\n",
    "    \"SSH-Patator\",\n",
    "    \"DoS Hulk\",\n",
    "    \"DoS Slowhttptest\",\n",
    "    \"DoS GoldenEye\",\n",
    "    \"DoS Slowloris\",\n",
    "    \"Portscan\",\n",
    "    \"DDoS\"\n",
    "]\n",
    "\n",
    "# ✅ 데이터 필터링 (`Label` 기준)\n",
    "df_filtered = df[df['Label'].isin(selected_classes)].copy()\n",
    "\n",
    "# ✅ X, y 분리 (라벨 인코딩 없이 원본 그대로 유지)\n",
    "X = df_filtered.drop(columns=['Label'])  # Feature Data\n",
    "y = df_filtered['Label']  # Target Labels\n",
    "\n",
    "# ✅ 클래스별 개수 확인\n",
    "print(\"Before SMOTE:\", Counter(y))\n",
    "\n",
    "# ✅ BENIGN 클래스 개수 확인\n",
    "benign_count = Counter(y)[\"BENIGN\"]  # BENIGN 샘플 개수\n",
    "\n",
    "# ✅ 기존 클래스 개수 저장\n",
    "target_stats = Counter(y)\n",
    "\n",
    "# ✅ SMOTE 비율 설정 (BENIGN 개수의 0.75 비율로 맞춤)\n",
    "sampling_strategy = {}\n",
    "\n",
    "for cls, count in target_stats.items():\n",
    "    if cls != \"BENIGN\":  # BENIGN 제외\n",
    "        new_count = int(benign_count * 0.75)  # 목표 개수 설정\n",
    "        if new_count > count:  # 기존 개수보다 클 때만 적용 (SMOTE는 over-sampling만 가능)\n",
    "            sampling_strategy[cls] = new_count\n",
    "\n",
    "print(\"SMOTE Sampling Strategy:\", sampling_strategy)\n",
    "\n",
    "# ✅ SMOTE 적용 (선택한 클래스만 over-sampling)\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# ✅ SMOTE 적용 후 클래스별 개수 확인\n",
    "print(\"After SMOTE:\", Counter(y_resampled))\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)  # 최신 sklearn 버전 호환\n",
    "y_onehot = onehot_encoder.fit_transform(y_resampled.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# ✅ 변환된 데이터프레임 생성\n",
    "df_resampled_onehot = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "\n",
    "# ✅ One-Hot 인코딩된 라벨 추가\n",
    "onehot_columns = onehot_encoder.get_feature_names_out(['Label'])\n",
    "df_onehot_labels = pd.DataFrame(y_onehot, columns=onehot_columns)\n",
    "\n",
    "# ✅ 최종 데이터프레임 결합\n",
    "df_resampled_onehot = pd.concat([df_resampled_onehot, df_onehot_labels], axis=1)\n",
    "\n",
    "# ✅ 최종 데이터 확인\n",
    "print(df_resampled_onehot.head())\n",
    "\n",
    "\n",
    "# ✅ One-Hot Encoding 확인\n",
    "print(\"One-Hot Encoded Labels:\")\n",
    "print(df_onehot_labels.head())\n",
    "\n",
    "# ✅ 원-핫 인코딩된 최종 데이터프레임 저장\n",
    "df_resampled_onehot.to_csv(\"resampled_data_onehot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c470999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_resampled_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1798822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8224b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ One-Hot Encoded Labels 포함된 데이터 사용\n",
    "tmp = df_resampled_onehot.drop(labels=df_onehot_labels.columns, axis=1)  # 라벨 컬럼 제외\n",
    "labels_onehot = df_resampled_onehot[df_onehot_labels.columns]  # 원-핫 인코딩된 라벨만 따로 저장\n",
    "\n",
    "# ✅ 라벨을 정수 인코딩으로 변환 (PCA 시각화를 위해)\n",
    "labels_int = labels_onehot.idxmax(axis=1)  # 가장 큰 값의 컬럼명 가져오기\n",
    "\n",
    "# ✅ 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(tmp)\n",
    "\n",
    "# ✅ PCA 적용 (주성분 개수 설정)\n",
    "n_components = 40  # 원하는 차원 수\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# ✅ PCA 누적 분산 비율 확인\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "print(\"누적 분산 비율 (cumulative explained variance):\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "# ✅ PCA 결과를 DataFrame으로 변환하고 label 병합\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "df_merged = X_pca_df.assign(label=labels_int.values)\n",
    "\n",
    "# ✅ 병합된 데이터 타입 확인\n",
    "print(\"Label 데이터 타입:\", df_merged['label'].dtype)\n",
    "\n",
    "# ✅ PCA 시각화 (2D로 줄인 경우)\n",
    "if n_components >= 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pd.factorize(labels_int)[0], cmap='viridis', alpha=0.5)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('PCA 결과 시각화')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91c3b4-b3c5-49fc-b0dc-c33911481691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00498d-17a2-4b1d-863a-0b49859c7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ✅ 라벨을 다시 One-Hot Encoding으로 변환\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "labels_reshaped = df_merged['label'].str.replace('Label_', '').to_numpy().reshape(-1, 1)  # 'Label_' 제거 후 변환\n",
    "y_onehot = onehot_encoder.fit_transform(labels_reshaped)\n",
    "\n",
    "# ✅ 변환된 One-Hot Labels를 DataFrame으로 변환 후 결합\n",
    "onehot_columns = onehot_encoder.get_feature_names_out(['Label'])\n",
    "df_onehot_labels = pd.DataFrame(y_onehot, columns=onehot_columns)\n",
    "\n",
    "# ✅ 원래 데이터에서 `label` 제거 후 결합\n",
    "df_final = df_merged.drop(columns=['label']).reset_index(drop=True)\n",
    "df_final = pd.concat([df_final, df_onehot_labels], axis=1)\n",
    "\n",
    "# ✅ 결과 확인\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ab889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4baad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c32652",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "tmp = df.drop(labels = 'label',axis=1)\n",
    "\n",
    "# 1. 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(tmp)\n",
    "\n",
    "# 2. PCA 적용 (주성분 개수 설정)\n",
    "n_components = 15  # 원하는 차원 수로 설정\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# 변환된 데이터 확인\n",
    "print(f'Original shape: {df.shape}')\n",
    "print(f'Transformed shape (PCA applied): {X_pca.shape}')\n",
    "\n",
    "# X_pca를 DataFrame으로 변환\n",
    "X_pca_df = pd.DataFrame(X_pca)\n",
    "\n",
    "# `assign`으로 label 추가\n",
    "df_merged = X_pca_df.assign(label=df['label'].values)\n",
    "\n",
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c72dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d877f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 정상 데이터만 추출 (Label_BENIGN == 1)\n",
    "X_train_normal = X_train[X_train['Label_BENIGN'] == 1].drop(columns=[\n",
    "    'Label_BENIGN', 'Label_DDoS', 'Label_DoS GoldenEye', 'Label_DoS Hulk',\n",
    "    'Label_DoS Slowhttptest', 'Label_DoS Slowloris', 'Label_FTP-Patator',\n",
    "    'Label_Portscan', 'Label_SSH-Patator'\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# ✅ 확인\n",
    "print(X_train_normal.shape)  # 정상 데이터 개수 확인\n",
    "print(X_train_normal.head())  # 데이터 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f2d98-52bb-4aa1-8ed6-9f006f75540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c799d8-0eb3-4667-9d88-d5145b7c002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ y_train_normal 생성 (정상 데이터의 라벨: BENIGN → 0)\n",
    "y_train_normal = X_train[X_train['Label_BENIGN'] == 1]['Label_BENIGN'].reset_index(drop=True)\n",
    "\n",
    "# ✅ y_train_normal 값을 0으로 변환 (정상 데이터는 0으로 설정)\n",
    "y_train_normal = y_train_normal.replace(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector, Conv1D, Conv1DTranspose\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras_self_attention import SeqWeightedAttention, SeqSelfAttention\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, Model, Input\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "latent_dim = 10\n",
    "inter_dim = 20\n",
    "\n",
    "# Sampling function for reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    batch_size = tf.shape(z_mean)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(0.5 * z_log_sigma) * epsilon  # z_log_sigma 대신 exp(0.5 * log_sigma) 사용하여 std 적용\n",
    "\n",
    "# Bernoulli loss + KL Divergence loss\n",
    "def bernoulli_vae_loss(x, x_decoded_prob, z_mean, z_log_sigma):\n",
    "    # BCE Loss for reconstruction (Bernoulli likelihood)\n",
    "    reconstruction_loss = losses.binary_crossentropy(x, x_decoded_prob)  # Bernoulli 기반 Binary Cross-Entropy 사용\n",
    "    reconstruction_loss = K.sum(reconstruction_loss, axis=-1)  # Sum over features\n",
    "\n",
    "    # KL Divergence loss\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    kl_loss_weighted = kl_loss * 0.0001  # KL 손실 가중치 적용\n",
    "    \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss_weighted)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# ✅ CVAE Model with fixed reshape issue\n",
    "def cvae(X, labels):\n",
    "    features = X.shape[1]  # 40 features from PCA\n",
    "    input_x = Input(shape=(features,), name='InputFeatures')\n",
    "    input_label = Input(shape=(1,), name='InputLabel')\n",
    "\n",
    "    # Convert label to categorical (One-Hot Encoding-like representation)\n",
    "    embedded_label = layers.Dense(features, activation='relu')(input_label)  # Output shape: (batch_size, 40)\n",
    "\n",
    "    # Concatenate input_x and embedded_label\n",
    "    concatenated_input = layers.Concatenate()([input_x, embedded_label])  # Shape: (batch_size, 80)\n",
    "\n",
    "    # Reshape for Conv1D layer (adjusted to correct shape)\n",
    "    reshaped_input = layers.Reshape((features * 2, 1))(concatenated_input)  # Shape: (batch_size, 80, 1)\n",
    "\n",
    "    # CNN Encoder\n",
    "    h = layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same')(reshaped_input)\n",
    "    h = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same')(reshaped_input)\n",
    "    h = layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\", padding='same')(reshaped_input)\n",
    "    h = layers.Flatten()(h)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim)(h)\n",
    "    z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "    # Decoder\n",
    "    decoder1 = layers.Dense((features * 2) * 16, activation='relu')(z)  # Match input shape\n",
    "    decoder1 = layers.Reshape((features * 2, 16))(decoder1)\n",
    "\n",
    "    decoder1 = layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "    decoder1 = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "    decoder1 = layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "\n",
    "    # Flatten and final output layer\n",
    "    decoder1 = layers.Flatten()(decoder1)\n",
    "    decoder1 = layers.Dense(features, activation=\"sigmoid\")(decoder1)  # Bernoulli output\n",
    "\n",
    "    model = Model([input_x, input_label], decoder1)\n",
    "    model.add_loss(bernoulli_vae_loss(input_x, decoder1, z_mean, z_log_sigma))\n",
    "\n",
    "    return model\n",
    "\n",
    "# ✅ Create Bernoulli-based CVAE model\n",
    "model = cvae(X_train_normal, y_train_normal)\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), metrics=['accuracy'])\n",
    "\n",
    "# EarlyStopping 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_normal, y_train_normal], X_train_normal,\n",
    "                    shuffle=True,\n",
    "                    epochs=50, \n",
    "                    validation_split=0.1,  \n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping]).history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b39cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training losses\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e554065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 Reconstruction Error 계산\n",
    "valid_x_predictions = model.predict([X_test, y_test])\n",
    "\n",
    "# Bernoulli 예측은 확률로 출력되므로, 0.5 기준으로 이진화\n",
    "valid_x_predictions_binary = (valid_x_predictions > 0.5).astype(int)\n",
    "\n",
    "# Reconstruction error 계산\n",
    "mse = np.mean(np.power(X_test - valid_x_predictions_binary, 2), axis=1)\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb37e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df['reconstruction_error']=error_df['reconstruction_error']*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df['true_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01287601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_df[error_df['true_class']==1]['reconstruction_error'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf67741",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_1 = error_df[error_df['true_class']==1]['reconstruction_error'].quantile(.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff98523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q4_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55977920",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_0 = error_df[error_df['true_class']==0]['reconstruction_error'].quantile(.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df[error_df['true_class']==0]['reconstruction_error'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fbce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터프레임 준비\n",
    "true_class = error_df['true_class'].astype(str)\n",
    "reconstruction_error = error_df['reconstruction_error']\n",
    "\n",
    "# 박스 플롯 그리기\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.boxplot(\n",
    "    [reconstruction_error[true_class == cls] for cls in sorted(true_class.unique())],\n",
    "    labels=sorted(true_class.unique()),\n",
    "    showfliers=False,\n",
    "    vert=True,\n",
    "    patch_artist=True\n",
    ")\n",
    "\n",
    "plt.ylabel('Distribution')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642257de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ✅ 실제 값 (true_class) & 예측 값 설정\n",
    "y_true = error_df['true_class']\n",
    "y_pred = error_df['predicted_class']  # predicted_class는 이미 threshold 기반으로 설정했다고 가정\n",
    "\n",
    "# ✅ Confusion Matrix 계산\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ✅ Confusion Matrix 시각화 (다중 클래스)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=sorted(y_true.unique()), yticklabels=sorted(y_true.unique()))\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"Actual Class\")\n",
    "plt.title(\"Multi-Class Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ✅ Precision, Recall, F1-score 계산 (다중 클래스)\n",
    "report = classification_report(y_true, y_pred, target_names=[str(c) for c in sorted(y_true.unique())])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a64c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ✅ 라벨 Binarize (One-vs-Rest 방식)\n",
    "classes = sorted(y_true.unique())  # 클래스 목록\n",
    "y_true_bin = label_binarize(y_true, classes=classes)\n",
    "y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "\n",
    "# ✅ ROC Curve 계산\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, class_label in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Class {class_label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# ✅ 대각선 기준선 추가\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "# ✅ 그래프 설정\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Multi-Class ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ✅ 실제 값 & 예측 값 설정\n",
    "y_true = error_df['true_class']\n",
    "y_pred = error_df['predicted_class']\n",
    "\n",
    "# ✅ Confusion Matrix 계산 (다중 클래스)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ✅ FPR 계산 (클래스별)\n",
    "fpr_matrix = np.zeros_like(conf_matrix, dtype=np.float64)  # FPR 행렬 초기화\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):  # 각 클래스별 계산\n",
    "    FP = conf_matrix[:, i].sum() - conf_matrix[i, i]  # False Positives (해당 열 합 - TP)\n",
    "    TN = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - conf_matrix[i, i])  # True Negatives\n",
    "    if FP + TN > 0:\n",
    "        fpr_matrix[i, :] = FP / (FP + TN)  # FPR 계산\n",
    "    else:\n",
    "        fpr_matrix[i, :] = 0  # 0으로 처리\n",
    "\n",
    "# ✅ FPR Confusion Matrix 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(fpr_matrix, annot=True, fmt=\".4f\", cmap=\"Blues\", xticklabels=sorted(y_true.unique()), yticklabels=sorted(y_true.unique()))\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"Actual Class\")\n",
    "plt.title(\"FPR-Based Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for plotting\n",
    "true_class_0 = error_df[error_df['true_class'] == 0]['reconstruction_error']\n",
    "true_class_1 = error_df[error_df['true_class'] == 1]['reconstruction_error']\n",
    "\n",
    "# Create the figure and axes with a specified y-axis limit\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Add jitter to avoid overlapping points in the scatter plot\n",
    "x_0 = np.random.normal(1, 0.04, size=len(true_class_0))  # Jitter for class 0\n",
    "x_1 = np.random.normal(2, 0.04, size=len(true_class_1))  # Jitter for class 1\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(x_0, true_class_0, color='orange', alpha=0.6, edgecolor='black', label='Class 0')\n",
    "plt.scatter(x_1, true_class_1, color='blue', alpha=0.6, edgecolor='black', label='Class 1')\n",
    "\n",
    "# Set y-axis limit\n",
    "plt.ylim(0, 3000)\n",
    "\n",
    "# Set x-axis labels and adjust ticks\n",
    "plt.xticks([1, 2], ['0', '1'])\n",
    "plt.xlabel('True Class')\n",
    "\n",
    "# Set y-axis label and title\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error by True Class')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for plotting\n",
    "true_class_0 = error_df[error_df['true_class'] == 0]['reconstruction_error']\n",
    "true_class_1 = error_df[error_df['true_class'] == 1]['reconstruction_error']\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Create a swarm plot equivalent using scatter plot with more jitter\n",
    "x_0 = np.random.normal(1, 0.1, size=len(true_class_0))  # Increased jitter for class 0\n",
    "x_1 = np.random.normal(2, 0.1, size=len(true_class_1))  # Increased jitter for class 1\n",
    "\n",
    "ax.scatter(x_0, true_class_0, color='blue', alpha=0.6, label='Class 0', edgecolor='w', s=50)\n",
    "ax.scatter(x_1, true_class_1, color='orange', alpha=0.6, label='Class 1', edgecolor='w', s=50)\n",
    "\n",
    "# Create boxplots\n",
    "ax.boxplot([true_class_0, true_class_1], positions=[1, 2], widths=0.6, patch_artist=True, \n",
    "           showfliers=False, boxprops=dict(facecolor='None', color='black'),\n",
    "           medianprops=dict(color='black'), whiskerprops=dict(color='black'))\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Class 0', 'Class 1'])\n",
    "\n",
    "# Add a horizontal line at the threshold\n",
    "ax.axhline(y=0.03, xmin=0.0, xmax=1, dashes=(5, 5), color='red')\n",
    "\n",
    "# Adjust y-axis limit to make sure the threshold line is visible\n",
    "#ax.set_ylim(bottom=min(min(true_class_0), min(true_class_1)) - 0.01, \n",
    "#           top=max(max(true_class_0), max(true_class_1)) + 0.01)\n",
    "ax.set_ylim(bottom=0, top=3000)\n",
    "    \n",
    "# Set labels and title\n",
    "ax.set_ylabel('Reconstruction Error')\n",
    "ax.set_title('Reconstruction Error by Class')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the range of threshold values\n",
    "threshold_range = np.arange(2590, 2650, 1)\n",
    "\n",
    "# List to store F1 scores for each threshold\n",
    "f1_scores = []\n",
    "\n",
    "# Loop through each threshold and calculate F1 score\n",
    "for threshold in threshold_range:\n",
    "    y_pred = [0 if e < threshold else 1 for e in error_df.reconstruction_error.values]\n",
    "    f1 = f1_score(error_df.true_class, y_pred)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the threshold with the highest F1 score\n",
    "best_threshold = threshold_range[np.argmax(f1_scores)]\n",
    "best_f1_score = max(f1_scores)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best F1 score: {best_f1_score}\")\n",
    "\n",
    "# Optionally, you can plot the F1 scores across the threshold range\n",
    "plt.plot(threshold_range, f1_scores, marker='o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea29dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 보고 threshold 결정 42.19124597192374\n",
    "threshold = 2590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599142ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='^', ms=3.5, linestyle='',\n",
    "            label= \"Normal\" if name == 0 else \"Fall\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.set_ylim(0, 40)\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction ierror for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d433bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have already defined LABELS, threshold, and y_pred\n",
    "\n",
    "LABELS = [\"Attack\", \"Normal\"]\n",
    "\n",
    "y_pred = [0 if e < threshold else 1 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(12, 12))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the confusion matrix using imshow\n",
    "cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(cax)\n",
    "\n",
    "# Set the labels for the axes\n",
    "ax.set_xticks(np.arange(len(LABELS)))\n",
    "ax.set_yticks(np.arange(len(LABELS)))\n",
    "\n",
    "ax.set_xticklabels(LABELS)\n",
    "ax.set_yticklabels(LABELS)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Predicted class')\n",
    "plt.ylabel('True class')\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "# Annotate the confusion matrix with the counts\n",
    "for i in range(len(LABELS)):\n",
    "    for j in range(len(LABELS)):\n",
    "        ax.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2 else \"black\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57341cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "precision, recall, f1,_ = precision_recall_fscore_support(y_test,y_pred,average='binary')\n",
    "print ('Accuracy Score :',accuracy_score(error_df.true_class, y_pred) )\n",
    "print ('Precision :',precision )\n",
    "print ('Recall :',recall )\n",
    "print ('F1 :',f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00630698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assuming y_test and reconstruction_error have been defined\n",
    "fpr, tpr, thresholds = roc_curve(y_test, reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Extract the true labels and reconstruction errors from error_df\n",
    "y_true = error_df['true_class']\n",
    "y_scores = error_df['reconstruction_error']\n",
    "\n",
    "# Compute the False Positive Rate (FPR) and True Positive Rate (TPR) for different thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "# Compute the area under the ROC curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd559432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If error_df.true_class is a nested structure, flatten it\n",
    "y_test = error_df.true_class.apply(lambda x: int(x[0][0]) if isinstance(x, list) else int(x))\n",
    "\n",
    "# Ensure y_pred is a list of integers\n",
    "y_pred = [0 if e < threshold else 1 for e in error_df.reconstruction_error.values]\n",
    "\n",
    "# Now calculate the metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718404f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3c82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08efaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
